\section{Experiments}
We assess the performance of PSRs and different kinds of Multi-PSRs on labyrinth environments. We look to see how performance varies as parameters are varied. Parameters include the model size, the number of observations used, and the type of environment. For all the plots, the x-axis is model size of the PSR/M-PSRs and the y-axis is an error measurement of the learned PSR/M-PSRs.

\subsection{Obtaining Observation Sequences}
In all the experiments we consider, an agent is positioned in a starting location and stochastically navigates the environment based on transition probabilities between states. When state-to-state transitions occur an observation symbol is produced. When the agent exits the labyrinth, we say the trajectory is finished, and we record the concatenation of the symbols produced. We call this concatenation the observation sequence for that trajectory.  

\subsection{Learning Implementation: Timing v.s Multiple Symbols}

For the timing case, we construct our empirical hankel matrix by taking P,S = $\{\sigma^i, \forall i<=n\}$. The parameter n depends on the application. For Double Loop environments we set n = 150, while for the pacman labyrinth n = 600. For these choices of n, we verify that as the amount of data gets large the learned PSR with the true model size becomes increasingly close to the true model. For Base M-PSR, we set $\Sigma'$ to be {$\sigma^{2^k}, k<=256 $}.

For multiple observations a slightly more complex approach is required to choose P and S. For prefixes P, we select the k most frequent prefixes from our observations set. For suffixes S, we take all suffixes that occur from our set of prefixes. We also require prefix completeness. That is if p' is a prefix of p $\in P$, then p' $\in P$. This heuristic for constructing empirical hankel matrices was given in previous work by [] and it showed that (). For \textbf{Base M-PSR}, we set $\Sigma'$ to be $\{x^{2^k},\forall x \in \Sigma', k<=256 \}$. For the \textbf{Tree M-PSR} we set L to 7.

\subsection{Measuring Performance}
For timing, the goal is to make predictions about how long the agent will survive the environment. One can also ask conditional queries such as how long the agent should expect to survive given that t seconds have elapsed

\begin{equation*}
f(\sigma^m|\sigma^n) = \dfrac{\alpha_{\lambda} \cdot A(\kappa(\sigma^m)) \cdot \alpha_\infty}{\alpha_\lambda \cdot (I-A_{\sigma})^{-1} \cdot \alpha_{\infty} }
\end{equation*}

The goal for the multiple observation labyrinths is to make predictions about seeing observation sequences. Conditional queries are also possible here 
\begin{equation*}
f(a^{m_1}b^{m_2}|a^{n_1}b^{n_2}) = \dfrac{\alpha_{\lambda} \cdot A(\kappa(a^{m_1}b^{m_2})) \cdot \alpha_\infty}{\alpha_\lambda \cdot A(\kappa(a^{n_1}b^{n_2})) \cdot \alpha_{\infty} }
\end{equation*}

To measure the performance of a PSR/M-PSR we use the following norm:
\begin{equation*}
||f - \hat{f}|| = \sqrt{\sum\nolimits_{x \in observations}(f(x) - \hat{f(x)})^2}
\end{equation*}
We use this norm because of a bound presented by [AUTHORS], which states that (). Here the function f denotes the true probability distribution over observations and the function $\hat{f}$ denotes the function associated with the learned M-PSR/PSR. In our environments, the function f is obtainable directly as we have access to the underlying HMMs.

Since the set of observations $\Sigma^*$ is infinite, we compute approximations to this error norm, by fixing a set of strings T and summing over T. For the timing case, we take T to be the $\{\sigma^k, \forall k<=n\}$, while for the multiple observation case, we take all possible strings producible from the prefixes and suffixes in our dataset. That is, for the multiple observation case $T = \{ps, \forall p \in P, \forall s \in S\}$.

\section{Double Loop Timing}

For timing, we start by considering a double loop environment. The lengths of the loops correspond to the number of states in the loop. A trajectory begins with the agent starting at the intersection of the two loops. At the intersection of the two loops, the agent has a 50 percent chance of entering either loop. At intermediate states in the loops the agent moves to the next state in the loop with probability 1-P and remains in its current state with probability P. Here, P represents the self-transition probability for internal states. Exit states are located halfway between each loop. At an exit state, the agent has a 50 percent probability of exiting the environment. 

\begin{figure}[ht!]
\centering
\includegraphics[width=60mm]{uCOREPICS/DL/doubleLoopImage.png}
\caption{Double Loop Environment\label{overflow}}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=60mm]{uCOREPICS/DL/64-16-10000.png}
\caption{Double Loop 64-16\label{overflow}}
\end{figure}

\subsection{Number of Trajectories}

Here, we vary the number of observations used in our dataset. PSRs/M-PSRs learned in Figure 4 use 100 observation sequences, while those in Figure 5 use 10000. In both cases the M-PSRs outperform the standard PSR for reduced model sizes.

\begin{figure}[ht!]
\centering
\includegraphics[width=60mm]{uCOREPICS/DL/64-16-100.png}
\caption{Double Loop 64-16\label{overflow}}
\end{figure}

\subsection{Noise: P}

Next, we vary the self-transition probability P to simulate noise in an environment. Figure 5 is a 64-16 double loop with P=0.2, and Figure 6 is a 64-16 double loop with P=0. We find that the noisy loops are more compressible, but the performance is worse for higher models. Nevertheless, M-PSRs still significantly outperform the standard PSR for reduced model sizes.

\begin{figure}[ht!]
\centering
\includegraphics[width=60mm]{uCOREPICS/DL/NoiseInfo.png}
\caption{Double Loop 64-16\label{overflow}}
\end{figure}

\subsection{Loop Lengths}

So far we have been using a 64-16 Double Loops. Here observations will come in low multiples of large powers of two. Intuitively, in this case the Base M-PSR should shine the most. In Figure 8, we plot the results of a 47-27 labyrinth where observations will not be so easily expressed from the Base M-PSR. Once again, M-PSRs outperform the standard PSR for reduced model sizes. In addition we see that the Data-Driven M-PSR does better than the Base M-PSR.

\begin{figure}[ht!]
\centering
\includegraphics[width=60mm]{uCOREPICS/DL/47-27-10000.png}
\caption{Double Loop 47-27\label{overflow}}
\end{figure}

\subsection{Choice of K for Base M-PSRs}

Below we plot performance of Base M-PSRs with different values of k. We note that as higher powers of operators are included performance improves.

\begin{figure}[ht!]
\centering
\includegraphics[width=60mm]{uCOREPICS/DL/basePows.png}
\caption{Double Loop 64-16\label{overflow}}
\end{figure}

\section{Large Labyrinth Timing}

We proceed to work with a more complex labyrinth environment. Figure 10 shows it's graphical representation. We test a larger environment as results in such a system would transfer to applications such as pacman. Transitions to new states occur with equal probability. The weight between transitions corresponds to the number of time steps. We add an additional parameter sF: stretchFactor, which multiplies all of the weights in the graph. 

\begin{figure}[ht!]
\centering
\includegraphics[width=60mm]{uCOREPICS/Pacman/Pacman10k.png}
\caption{Pacman Labyrinth\label{overflow}}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=40mm,height=60mm]{uCOREPICS/Pacman/graphPacMan.png}
\caption{Graph of pacman\label{overflow}}
\end{figure}

\subsection{Number of Trajectories}

In Figures 11 and 12 we vary the number of observations used for learning. First we note that performance of all models is significantly worse for less data. Nevertheless, M-PSRs outperform the traditional PSR regardless.

\begin{figure}[ht!]
\centering
\includegraphics[width=60mm]{uCOREPICS/Pacman/Pacman500.png}
\caption{Pacman Labyrinth\label{overflow}}
\end{figure}

\subsection{Stretch Factor: sF}

In Figures 13 and 14 we vary the stretch factor parameter. We find that a higher values of sF allow for increased improvement of the M-PSR relative to the performance of the standard PSR.

\begin{figure}[ht!]
\centering
\includegraphics[width=60mm]{uCOREPICS/Pacman/PacmanSF=1.png}
\caption{Stretch Factor: 1\label{overflow}}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=60mm]{uCOREPICS/Pacman/PacmanSF=5.png}
\caption{Stretch Factor: 5\label{overflow}}
\end{figure}

\section{Multiple Observations: Colored Loops}

We now move to the multiple observation case. Here the Data-Driven M-PSRs really show their strength as observation sequences are more complex. We construct a Double Loop environment where one loop is green and the other is blue. The lengths of each loop are also varied, see Figure 4 and Figure 5. We fix the length of observations to be $TrajectoryLength := (len(loop1) + len(loop2))*3$. To build empirical estimates of probabilities we set

\begin{equation*}
 f(x)=\dfrac{\#occurances of x}{counts(s \in Obs, len(s)>=x)}
\end{equation*}  This means that the PSRs will compute the probability of x occurring as a prefix.

\begin{figure}[ht!]
\centering
\includegraphics[width=60mm]{uCOREPICS/DLMO/MO_1k.png}
\caption{Colored Loops 27-17\label{overflow}}
\end{figure}

\lucas{Removed picture of colored loops. Waste of space and leaves too much evidence of MS paint.}

\subsection{Number of Trajectories}

As for the timing case, we vary the amount of data to learn PSRS/M-PSRs in Figures 15 and 16. Once again we find M-PSRs perform far better, especially the Data-Driven M-PSR. This makes sense as when complexity in observations increases only custom M-PSRs will express transitions compactly.

\begin{figure}[ht!]
\centering
\includegraphics[width=60mm]{uCOREPICS/DLMO/MO_50.png}
\caption{Colored Loops 27-17\label{overflow}}
\end{figure}



\subsection{Data Driven Sequences}
For the double loop case the greedy approach learns multiples of the loop lengths which results in partitions which use fewer operators. As an example, for the 47-27 labyrinth the greedy heuristic picked the following operators: $\Sigma'=\{\sigma^{47}, \sigma^{27}, \sigma^{74}, \sigma^{94} ...\}$. For Pacman, the learned strings are multiples of the stretch factor. For Colored Double Loops consistent operators in $\Sigma'$ are $\{g^{27},b^{17},g^{27}b^{17},b^{17}g^{27}\}$. Figure 19,20,and 21 show performance as we include more operators.

\begin{figure}[ht!]
\centering
\includegraphics[width=60mm]{uCOREPICS/DLMO/numOpComparison.png}
\caption{Colored Loops 27-17\label{overflow}}
\end{figure}
