\section{M-PSR: Definition and Learning}

\subsection{Multi-Step PSR}

A linear \emph{predictive state representation} for an autonomous dynamical system with discrete observations in a set $\Sigma$ is a tuple $\psrA = \psrsigma$ where: BLA BLA.

To define our model for multi-step PSR we basically augment a PSR with two extra objects: a set of \emph{multi-step observations} $\Sigma' \subset \Sigma^+$ containing non-empty strings formed by basic observations, and a \emph{coding function} $\kappa : \Sigma^\star \to {\Sigma'}^{\star}$ that given a string of basic observations produces an equivalent string composed using multi-step observations.
%
The choice of $\Sigma'$ and $\kappa$ can be quite application-dependent, in order to reflect the particular patterns arising from different environments. However, we assume this objects satisfy a basic set of requirements for the sake of simplicity and to avoid degenerate situations:
\begin{enumerate}
\item The set $\Sigma'$ must contain all symbols in $\Sigma$; i.e.\ $\Sigma \subseteq \Sigma'$
\item The function $\kappa$ satisfies $\partial(\kappa(x)) = x$ for all $x \in \Sigma^\star$, where $\partial : {\Sigma'}^\star \to \Sigma^\star$ is the \emph{decoding morphism} between free monoids given by $\partial(z) = z \in \Sigma^\star$ for all $z \in \Sigma'$. Note this implies that $\kappa(\epsilon) = \epsilon$, $\kappa(\sigma) = \sigma$ for all $\sigma \in \Sigma$, and $\kappa$ is injective.
\end{enumerate}

Using these definitions, a \emph{multi-step PSR} (M-PSR) is a tuple $\psrA' = \mpsrsigma$.

\subsection{Spectral Learning Algorithm}

We extend \cite{bootspsr}...

\subsection{Examples of M-PSRs}

%\textbf{Base PSR for Duration Models} BLA BLA...

In this section we go through examples of Multi-PSRs which are relevant to our experiment section. 

For the timing case one can use a M-PSR which we call the \textbf{Base M-PSR}. For the Base M-PSR, $\Sigma'$ is  $\{a^{2^k} \forall k <= n\}$. For the encoding map we write an observation $a^m = a^{2^n1} ... \cdot a^{2^nf}$.  With this equality we use the natural encoding map $\kappa(a^n) = \{a^{2^n_1},...,\{a^{2^n_f}\}$. 

For the multiple observation case one can also use The Base System. In this case $\Sigma' = \{\sigma^{2^k}\forall \sigma \in \sum, \forall k <= n\}$. For the encoding map $\kappa$ we first split the string into sequences of a fixed symbol and then use the same encoding as for timing. For example $\kappa(a^5b^3)=\{a^4,a,b^2,b\}$.
 
Another example for the multiple observation case is an M-PSR we will call the \textbf{Tree M-PSR}. For the Tree System, we set $\Sigma'= \{s \in \Sigma^{*}, len(s)<= L\}$. Here L is a parameter of choice. For the decoding map $\kappa$, we first split a string x as $x=x_1x_2...x_ny$, where $|x_i|=L, \forall i<=n$ and $|y|=len(x) - (n \cdot L)$. With this we set $\kappa(x) = \{x_1,x_2,...,x_n,y\}$.

The constructions of the M-PSRs above are not dependent on the environment. In our results section, we find that performance of M-PSRs depend heavily on how $\Sigma'$ reflects the observations in one's environment. Thus, we develop an algorithm for choosing $\Sigma'$. In addition, we provide a $\kappa$ which one can apply to any M-PSR and which delivers good experimental performance. Together, these yield another type of M-PSR, which we call a \textbf{Data-Driven M-PSR}.
