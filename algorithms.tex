\section{Learning Algorithm for M-PSR}

In this section, we describe a learning algorithm for M-PSR which combines the standard spectral algorithm for PSR \cite{bootspsr} with a data-driven greedy algorithm for building an extended set of symbols $\Sigma'$ containing frequent patterns that minimise a coding cost for a general choice of coding function $\kappa$.

\subsection{Spectral Learning Algorithm}

We extend \cite{bootspsr} to M-PSR under the assumption that $\kappa$ and $\Sigma'$ are given.

\borja{Need to fill this. Probably copy-paste from the ODM paper will do.}

\subsection{Notation}

\borja{We should move this into the two subsections below}

\textbf{Obs}: A mapping from observation sequences to the number of occurrences of that sequence in one's dataset. 

\textbf{SubObs} : all substrings of \textbf{Obs}.

\textbf{Q}: A query string (a string for which one wishes to determine the probability of).

\textbf{bestE}: A map from indices i of Q to the optimal encoding of Q[:i].

\textbf{minE}: A map from indices i of Q to $|bestE[i]|$

\textbf{opEnd}: A map from indices i of Q to the set of strings in $\Sigma'$: $\{x \in \Sigma' s.t Q[i-x.length:i] == x\}$

\textbf{numOps}: The desired number of operators one wants in $\Sigma'$. I.e numOps =  $|\Sigma'|$

\subsection{A General Coding Function}

Here we provide a dynamic programming algorithm which can serve as $\kappa$ for any M-PSR. Given a query string Q, and a set of transition sequences $\Sigma'$, the algorithm minimizes the number of sequences used in the partition $\kappa(Q)$. In other words, the algorithm minimizes $|\kappa(Q)|$. For the single observation case, the algorithm is equivalent to the coin change problem.

For a given string Q, the algorithm inductively computes the optimal string encoding for the prefix Q[:i]. It does so by minimizing over all $s \in \Sigma'$ which terminate at the index i of Q.

\begin{algorithm}
\caption{Encoding Algorithm}
\label{Encoding Algorithm}
\begin{algorithmic}[1]
\Procedure{DPEncode}{}

\State $bestE[] \gets new String[len(Q)+1]$
\State $minE[] \gets new Int[len(Q)+1]$
\State $opEnd[] \gets new String[len(Q)+1][]$

\State $bestEnd[0] = Q[0]$
\State $minE[0] = 0$

\For{i in range[1,Q.length]}
	 \State $opEnd[i] \gets \{s \in \Sigma', Q[i-len(s):i] == s\}$
\EndFor

\For{i in range[1,Q.length]}
	\State $bestOp \gets null$
	\State $m \gets null$ 
	\For{$s \in opEnd[i]$}
		\State $tempInt \gets minE[i-len(s)] + 1$
		\If{$m == null$ or $tempInt < m$}
			\State $m \gets temp$ 
			\State $bestOp \gets s$
		\EndIf
	\EndFor
	\State $minE[i+1] \gets m$
	\State $bestE[i+1] \gets bestE[i-len(bestOp)] + bestOp$
\EndFor

\Return $bestE[len(Q)]$

\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Greedy Selection of Multi-Step Observations}

Here we present a greedy heuristic which learns the multi-step transition sequences $\Sigma'$ from observation data. Having a $\Sigma'$ which reflects the types of observations produces by one's system will allow of short encodings when coupled with our a dynamic programming encoding algorithm. In practice, this greedy algorithm will pick substrings from one's observation set which are long, frequent, and diverse. From an intuitive standpoint, one can view structure in observation sequences as relating to the level of entropy in the system's observations. 

As a preprocessing step, we reduce the space of substrings textbf{subObs} to the k most frequent substrings in our observation set. Here frequent means the number of observation sequences a given substring $s \in subObs$ occurs in. 
\lucas{Added substring preprocessing step}

The algorithm evaluates substrings based on how much they reduce the number of transition operators used on one's observation data. The algorithm adds the best operator iteratively with $\Sigma'$ initialized to $\Sigma$. More formally at the i'th iteration of the algorithm the following is computed: $min_{sub \in SubObs} \sum\nolimits_{obs \in Obs}|\kappa(obs,\Sigma'_i \cup sub)|$. The algorithm terminates after the \textbf{numOps} iterations. 

\begin{algorithm}
\caption{Base Selection Algorithm}
\label{Base Selection}
\begin{algorithmic}[1]
\Procedure{Base Selection}{}
\State $\Sigma' \gets \{s, s \in \sum \}$
\State $Subs \gets \{$k frequent $s \in subObs\}$

\State $prevBestE \gets null$
\For{each obs in Obs}
	\State $prevBestEncoding[obs] \gets len(obs)$
\EndFor

\State $i\gets 0$\
\While{$i<numOperators$}
	\State $bestOp \gets null$
	\State $bestImp \gets null$
	\For{each s $\in Subs$ }
		\State $c \gets 0$
		\For{each obs in Obs}
			\State $c \gets c+DPEncode(obs)-prevBestE(obs)$
		\EndFor
		
		\If{$c>bestImp$}
			\State $bestOp \gets observation$
			\State $bestImp \gets c$
		\EndIf
		
	\EndFor

	\State $\Sigma' \gets \Sigma' \cup bestOp$
	\For{each obs in Obs}
		\State $prevBestE \gets DPEncode(obs,\Sigma'$) 
	\EndFor	
	
	\State $i \gets i + 1$
\EndWhile 
\Return $\Sigma'$

\EndProcedure
\end{algorithmic}
\end{algorithm}

