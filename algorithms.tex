\section{Fully Data-Driven Learning}

\subsection{Overview}

In the previous section we showed examples of Multi-PSRs whose $\Sigma$' and $\kappa$ parameters were independent of the environment. As the Multi-PSRs seem to offer improvements over the traditional PSR (results section), it would make sense to make these parameters a function of the observations of one's environment. 

\subsection{Learning Transition Operators}

Here we present a greedy heuristic which learns multi-step transition sequences $\Sigma'$ from observation data. Coupled with the algorithm for decomposition sequences, a good $\Sigma'$ will minimize the number of operators used when performing probability queries. While this is not the explicit goal, in practice the algorithm will typically pick substrings from one's observation set which are long, frequent, and diverse. From an intuitive standpoint, it can be useful to view the types of operators learned from an environment as a reflection of the degree of entropy in the observations that it produces. This is generally what we think of when we refer to structured environments.

The algorithm evaluates substrings based on how much they would reduce the number of transition operators used on one's observation data. The algorithm does this iteratively with $\Sigma' initialized to \sum$. More formally at the ith iteration of the algorithm it computes $min_{substring \in observations.substrings} of \sum k_(obs,\Sigma'_i \cup sub)$. The algorithm terminates after the Nth iteration. Here, N+|$\sum$| is a parameter of choice of the user corresponding to the desired size of $\Sigma'$.

\begin{algorithm}
\caption{Base Selection Algorithm}
\label{Base Selection}
\begin{algorithmic}[1]
\Procedure{Base Selection}{}
\State $BaseSystem \gets \{s, s \in \sum \}$
\State $bestSubstring \gets null$
\State $i\gets 0$\
\State $bestImprovement \gets null$
\While{$i<desiredBaseSize$}
	\For{each substring $s$ $\in Observations$ }
		\State $improvement \gets 0$
		\For{each observation $o$ in Observations}
			$improvement +=$ optimalOrder($o$) - previousBest($o$)
		\EndFor
		
		\If{$improvement>bestImprovement$}
			\State $bestSubstring \gets observation$
			\State $bestImprovement \gets improvement$
		\EndIf
		
	\EndFor

	\State $BaseSystem \gets BaseSystem \cup bestSubstrings$
	\State $bestSubstring \gets null$
	\State $bestImprovement \gets null$

\EndWhile
\Return $BaseSystem$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Learning the Encoding Function}

Here we provide a dynamic programming algorithm which can serve as $kappa$ for any M-PSR. Given a query string Q, and a set of transition sequences $\Sigma'$, the algorithm minimizes the number of sequences used in the partition. In other words, the algorithm minimizes $|k(Q)|$. For the single observation case, the algorithm is equivalent to the coin change problem.

For a given string S, the algorithm inductively computes the optimal string composition for the prefix s[:i]. It does so by minimizing over all $s \in \Sigma'$ which terminate at s[i].

\begin{algorithm}
\caption{Encoding Algorithm}
\label{Encoding Algorithm}
\begin{algorithmic}[1]
\Procedure{Encoding}{}

\State $Map<Int,String[]> bestPartition \gets null$
\State $sequencesEnding[] \gets new String[Q.length]$
\State $minPartition[] \gets new Int[Q.length]$

\State $minPartition[] \gets 0$
\State $sequencesEnding[0] \gets ""$

\For{i in range(Q.length)}
	 $sequencesEnding[i] \gets \{x \in \Sigma' s.t Q[i-x.length:i] == x\}$
\EndFor

\For{i in range(Q.length)}
	\State $bestOp \gets null$
	\State $bestPartition \gets null$ 
	\For{$s \in sequencesEnding[i]$}
		\State $partitionVal \gets minPartition[i-s.length]$
		\If{$bestPartition == null or partitionVal < bestPartition$}
			\State $bestPartition \gets partitionVal$ 
			\State $bestOp \gets s$
		\EndIf
	\EndFor
	\State $minPartition[i] \gets minPartition[i-bestOp.length] + 1$
	\State $bestPartition[i] \gets bestPartition.get(i).add(bestOp) $
\EndFor

\Return $bestPartition[Q.length]$

\EndProcedure
\end{algorithmic}
\end{algorithm}