\section{Fully Data-Driven Learning}

\subsection{Overview}

In this section, we provide two algorithms: the first being a general algorithm for $\kappa$ and the second for choosing $\Sigma'$ from Data. We present these algorithms together in the context of \textbf{Data-Driven M-PSRs} as the Data-Driven algorithm for choosing $\Sigma'$ makes use of the algorithm for our choice of $\kappa$.

\subsection{Notation}

\textbf{Obs}: A mapping from observation sequences to the number of occurrences of that sequence in one's dataset. 

\textbf{SubObs} : all substrings of \textbf{Obs}.

\textbf{Q}: A query string (a string for which one wishes to determine the probability of).

\textbf{bestE}: A map from indices i of Q to the optimal encoding of Q[:i].

\textbf{minE}: A map from indices i of Q to $|bestE[i]|$

\textbf{opEnd}: A map from indices i of Q to the set of strings in $\Sigma'$: $\{x \in \Sigma' s.t Q[i-x.length:i] == x\}$

\textbf{numOps}: The desired number of operators one wants in $\Sigma'$. I.e numOps =  $|\Sigma'|$

\subsection{Learning the Encoding Function}

Here we provide a dynamic programming algorithm which can serve as $\kappa$ for any M-PSR. Given a query string Q, and a set of transition sequences $\Sigma'$, the algorithm minimizes the number of sequences used in the partition $\kappa(Q)$. In other words, the algorithm minimizes $|\kappa(Q)|$. For the single observation case, the algorithm is equivalent to the coin change problem.

For a given string Q, the algorithm inductively computes the optimal string encoding for the prefix Q[:i]. It does so by minimizing over all $s \in \Sigma'$ which terminate at the index i of Q.

\begin{algorithm}
\caption{Encoding Algorithm}
\label{Encoding Algorithm}
\begin{algorithmic}[1]
\Procedure{DPEncode}{}

\State $bestE[] \gets null$
\State $minE[] \gets new Int[Q.length]$
\State $opEnd[] \gets new String[Q.length]$

\For{i in range(Q.length)}
	 \State $opEnd[i] \gets \{s \in \Sigma', Q[i-s.length:i] == s\}$
\EndFor

\For{i in range(Q.length)}
	\State $bestOp \gets null$
	\State $m \gets null$ 
	\For{$s \in opEnd[i]$}
		\State $tempInt \gets minE[i-s.length] + 1$
		\If{$m == null or tempInt < m$}
			\State $m \gets temp$ 
			\State $bestOp \gets s$
		\EndIf
	\EndFor
	\State $minE[i] \gets m$
	\State $bestE[i] \gets bestE[i-bestOp.length] + bestOp$
\EndFor

$ADD OFF BY ONE STUFF$

\Return $bestE[Q.length]$

\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Learning Transition Operators}

Here we present a greedy heuristic which learns the multi-step transition sequences $\Sigma'$ from observation data. Having a $\Sigma'$ which reflects the types of observations produces by one's system will allow of short encodings when coupled with our a dynamic programming encoding algorithm. In practice, this greedy algorithm will pick substrings from one's observation set which are long, frequent, and diverse. From an intuitive standpoint, one can view structure in observation sequences as relating to the level of entropy in the system's observations. 

The algorithm evaluates substrings based on how much they reduce the number of transition operators used on one's observation data. The algorithm adds the best operator iteratively with $\Sigma'$ initialized to $\Sigma$. More formally at the i'th iteration of the algorithm the following is computed: $min_{sub \in SubObs} \sum\nolimits_{obs \in Obs}|\kappa(obs,\Sigma'_i \cup sub)|$. The algorithm terminates after the \textbf{numOps} iterations. 

\begin{algorithm}
\caption{Base Selection Algorithm}
\label{Base Selection}
\begin{algorithmic}[1]
\Procedure{Base Selection}{}
\State $\Sigma' \gets \{s, s \in \sum \}$
\State $bestOp \gets null$
\State $i\gets 0$\
\State $bestImp \gets null$

\State $prevBestE \gets null$
\For{each obs in Obs}
	\State $prevBestEncoding[obs] \gets obs.length$
\EndFor

\While{$i<numOperators$}
	\For{each s $\in ObsSub$ }
		\State $temp \gets 0$
		\For{each obs in Obs}
			\State $temp \gets temp + DPEncode(obs) - prevBestE(obs)$
		\EndFor
		
		\If{temp$>$bestImp}
			\State $bestOp \gets observation$
			\State $bestImp \gets temp$
		\EndIf
		
	
		
	\EndFor

	\State $\Sigma' \gets \Sigma' \cup bestOp$
	\For{each obs in Obs}
		\State $prevBestE \gets DPEncode(obs,\Sigma'$) 
	\EndFor	
	
	\State $bestOp \gets null$
	\State $bestImp \gets null$
	\State $i \gets i + 1$
\EndWhile 
\Return $BaseSystem$

\EndProcedure
\end{algorithmic}
\end{algorithm}

